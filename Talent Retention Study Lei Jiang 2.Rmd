---
title: "Talent Retention Study"
author: "Lei Jiang"
date: "December 5, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Introduction

####DDSAnalytics is an analytics company that specializes in talent management solutions for Fortune 1000 companies. Talent management is defined as the iterative process of developing and retaining employees. It may include workforce planning, employee training programs, identifying high-potential employees and reducing/preventing voluntary employee turnover (attrition). To gain a competitive edge over its competition, DDSAnalytics is planning to leverage data science for talent management. The executive leadership has identified predicting employee turnover as its first application of data science for talent management. Before the business green lights the project, they have tasked your data science team to conduct an analysis of existing employee data. 

##Objectives

####Identify the top features that contribute to employee retention.

####Identify job role specific trends.

####Build classification models to predict employee turnover using existing employee data.

##Exploratory data analysis 

```{r}
DF <- read.csv("C:/Users/N1110/Desktop/CaseStudy2-data.csv",header=TRUE)

dim(DF)


str(DF) 
#9 factors, 27 integers, 1 random number

#Data Distribution

# Check Missing Data, there is no missing value
naCount<- colSums(is.na(DF))
naCount

#data imbalance, more than 80% No, less than 20% yes. attrition  yes means the employee left, no means retained, so most employee in this study retained.
# attrition level: yes 2 , no 1

table(DF$Attrition)
barplot(prop.table(table(DF$Attrition)), main="Imbalanced Data Distribution of Attrition in Training data set ")
```

###Objective 1. Identify (at least) the top three factors that contribute to turnover.
####Analysis of feature importance using correlation matrix was perfomed next.

#### By observing the data, we identified a number of varaibles are not related to our outcome, or constant for all observations and therefore has no value to be included in the correlation matrix analysis.

####Identifying the most important features for outcome AttritionInt from correlation matrix created by R, the process is described here:
To measure the correlation, we need to use relative importance rather than absolute ones since we can only pick features from the data we have.

 Features with absolute value of Pearson correlation coefficient equal or greater than 0.06 are as following: (positive correlation means bad effect to Attrition and vers versa)

 Age (-0.13), DailyRate (-0.07), DistanceFromHome (0.07), EnvironmentSatisfaction (-0.13) , JobInvolvement (-0.15) , JobLevel(-0.15) ,JobSatisfaction (-0.09),  MonthlyIncome (-0.14),StockOptionLevel (-0.14),
TotalWorkingYears(-0.15), WorkLifeBalance (-0.09), YearsAtCompany(-0.12), YearsInCurrentRole(-0.15),  YearsWithCurrManager(-0.14), TrainingTimesLastYear (-0.06), NumCompaniesWorked(0.06)


Then remove correlated features (multicollinearity, pairwise linear correlation):
because if multicollinearity exists, the coefficient estimations are not trustable. So we exclude correlated features, and only keep one most representitive one (can check p value in backward selection to decide which feature to keep).
by domain knowledge, we know monthlyIncome, dailyrate, hourlyrate(which only has 0.02 correlation, so not listed here) are highly correlated, so we only keep one of them that is most representitive: monthlyIncome; YearsAtCompany(-0.12), YearsInCurrentRole(-0.15), TotalWorkingYears(-0.15),  and YearsWithCurrManager(-0.14) are highly correlated, so we only keep one of them that is most represtitive and has largest correlation:YearsInCurrentRole(-0.15)  ; 
by checking the correlation matrix we may identify more features that correlated.

So after removing correlated features, we have these features highly correlated in the order of importance from high to low:

 JobInvolvement (-0.15) , JobLevel(-0.15) , YearsInCurrentRole(-0.15),  MonthlyIncome (-0.14),StockOptionLevel (-0.14),
Age (-0.13),  EnvironmentSatisfaction (-0.13) , WorkLifeBalance (-0.09),    JobSatisfaction (-0.09), DistanceFromHome (0.07),TrainingTimesLastYear (-0.06), NumCompaniesWorked(0.06)


Also notice that these are only quantitative features; there are features recorded as factors, like EducationField, Department (leves 1-3), BusinessTravel (leves 1-3), gender(levels 1,2), jobrole (9 levels), OverTime (levels 1,2), maritalStatus(levels 1,2,3) that we could not quantify. So in this step we did not consider them in. We may need to use OneHotEncoder on catagorical varabiles. 
From this finding during EDA, I think change all factor features to int when read.csv the data at the begining may be better. This is how I made the decision.
However, from here, we just change factor varaibles to int or numeric and check the correlation with our outcome. Remember we need to do the same thing to Validation DF in order to test our model.


(Trial: For the same reason of transformed "Attrition" to INT, "OverTime" and "Gender" could be transformed into INT and, test for correlation with "Attrition".

For "BusinessTravel", "Department", "EducationField", "JobRole", "MaritalStatus" I am not sure about how use One Hot Encoding to transform them. But I tried after converting into INT and test for correlation with "Attrition", we found features highly correlated with outcome are MaritalStatus (0.18), OverTime (0.24), and Department (0.08). So OverTime has great positive impact on Attrition and therefore great negative impact on retention (we can even create another outcome called "Retention" which is opposite of "Attrition", such as Attrition levels: yes 2 , no 1; Retention levels: yes 1 , no 2). We can argue that JobRole (0.06) might also be important, it is a judgemental call.

Our study found, Gender (0.05), and businessTravel (-0.02) actually do not have much impact on Attrition.)


In conclusion, in order to retain the talents for the company, the following features, JobInvolvement (-0.15) , Age (-0.13), JobLevel(-0.15) , YearsInCurrentRole(-0.15), MaritalStatus (0.18)  of the empoloyee are important but, are these are features the company can not control (unless they only hire more experienced people or not married people). For feature Department (0.08), we need every Departments in the company to operate, so we can not choose to hire people for only a certain Department. Thus, we can not work to improve this one either.  We can suggest the company to improve on features that the company can control on their side, such as MonthlyIncome (-0.14),StockOptionLevel (-0.14), EnvironmentSatisfaction (-0.13) , and WorkLifeBalance (-0.09).  So my conclusion is these four factors stated above are the most important factors I found that contribute to turnover from the dataset we were given.


Combine the two studies above together, we have these 16 features highly correlated in the order of importance from high to low might be useful to build our model for later:

OverTime (0.24), MaritalStatus (0.18),JobInvolvement (-0.15) , JobLevel(-0.15) , YearsInCurrentRole(-0.15),  MonthlyIncome (-0.14),StockOptionLevel (-0.14),
Age (-0.13),  EnvironmentSatisfaction (-0.13) , WorkLifeBalance (-0.09),    JobSatisfaction (-0.09), Department (0.08), DistanceFromHome (0.07), JobRole (0.06), TrainingTimesLastYear (-0.06), NumCompaniesWorked(0.06).

(OverTime (0.24) is kind of correlated to WorkLifeBalance (-0.09) by definition. However, after check the correlation matrix, we found it is not highly correlated.)

(This list also matches the importance plot we get from random forest model that will be disscuss in detail in the objective 3. )


I will choose these for top features, since they are listed top scores in both correlation matrix and RF model. Also, these featurescan be easily transferred to actionable items for the company.
OverTime (0.24), JobInvolvement (-0.15), JobLevel(-0.15), MonthlyIncome (-0.14), StockOptionLevel (-0.14)


```{r }
# For the correlation matrix, Remove columns that are not necessary EmployeeCount(all values are 1, not helpful in our study),  StandardHours (all values are 80), Over18(all value Y),
# EmployeeNumber? what does Rand mean?

DF2 <- subset(DF, select = -c(ID, EmployeeCount,StandardHours, Over18))

DF2$OverTimeINT  <- as.integer(DF$OverTime)
DF2$GenderINT  <- as.integer(DF$Gender)

#OneHotEncoding for Attrition. used "Int" in the name of transformed Attrition to label it as outcome. Later, some transformed features  will be labled as "INT".
DF2$AttritionInt <- as.integer(DF$Attrition) 
#after change Attrtition to Int, 1,2, 1 means yes, 2 means no. 
#so greater positive corr with AttritionInt means Attrition no->retained.?? 
# but result opposite in lower matrix part. therefore, so greater negative corr with AttritionInt means Attrition no->retained.


#choose only numeric columns
nums <- unlist(lapply(DF2, is.numeric)) 
DFnum<- DF2[ , nums]

# create correlation matrix, heatmap of correlation matrix is for visual reference when selecting the important features; we can not only rely on heatmap.
cormat <- round(cor(DFnum),2)
#head(cormat)


library(reshape2)
melted_cormat <- melt(cormat)
head(melted_cormat)


library(ggplot2)
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()+ggtitle("Pearson correlation coefficient matrix for all numeric variables")

# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }


upper_tri <- get_upper_tri(cormat)
upper_tri  # can choose to print out the whole table or not
  
  # Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
    cormat[upper.tri(cormat)] <- NA
    return(cormat)
  }
  
  lower_tri <- get_lower_tri(cormat)
#lower_tri   # can choose to print out the whole table or not

# str(DF2) #run this code, and according to the list, convert factor features into INT 
#(luckily there are not many, only 7 besides Attrition, so this approach is still feasible). to save some page, I did not print out this list.

DF2$BusinessTravelINT  <- as.integer(DF$BusinessTravel ) 
DF2$DepartmentINT  <- as.integer(DF$Department ) 
DF2$EducationFieldINT  <- as.integer(DF$EducationField ) 

DF2$JobRoleINT  <- as.integer(DF$JobRole)
DF2$MaritalStatusINT  <- as.integer(DF$MaritalStatus) 

  
  #create DF3 to contain all converted features
  
 DF3<- cbind(DF2$AttritionInt,DF2$BusinessTravelINT, DF2$DepartmentINT,DF2$EducationFieldINT, DF2$GenderINT ,DF2$JobRoleINT,DF2$MaritalStatusINT ,DF2$OverTimeINT )

 is.data.frame(DF3)
#[1] FALSE

DF3=as.data.frame(DF3)
is.data.frame(DF3)

names(DF3)<-c("Attrition","BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "OverTime")

  # create correlation matrix, heatmap of correlation matrix is for visual reference when selecting the important features; we can not only rely on heatmap.
  
  cormat <- round(cor(DF3),2)
#head(cormat)
melted_cormat <- melt(cormat)
head(melted_cormat)

#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  #geom_tile+()ggtitle("Pearson correlation coefficient matrix for")
  
  
# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }


upper_tri <- get_upper_tri(cormat)
#upper_tri  # can choose to print out the whole table or not

```

###Objective 2. Provide any other interesting jobrole specific trends and observations from your analysis.

####By identifying the lightest color area on the heatmap, we found JobRole is highly correlatd to WorkLifeBalance, RelationshipSatisfaction, EvironmentSatisfaction, HourlyRate, Education, StockOptionLevel,EmployeeNumber, PerformanceRating, JobSatisfaction,DistancFromHome, and JobInvolvement. 

####Interesting insights were found when we compared the average feature values by job roles for follwoing features: Department,JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance, Education, JobInvolvement, PerformanceRating and EnvironmentSatisfaction, to see the trends.

####The mean value of features are higher, the better, for all these features, except for Education (Levels: 1 'Below College', 2 'College', 3 'Bachelor', 4 'Master', 5 'Doctor') and Department (Levels:1 Human Resources, 2 Research & Development, 3 Sales).

levels stand for JobRole :
JobRole                 : Factor w/ 9 levels
1: Healthcare Representative
2: Human Resources
3: Laboratory Technician
4: Manager
5: Manufacturing Director 
6: Research Director 
7: Research Scientist                
8: Sales Executive 
9: Sales Representative 
 
####In Objective 2, we want JobRole specific trends.  So we plot correlation of features vs. JobRole to identify the most correlated features to JobRole.


```{r }


#choose only numeric columns for updated DF2
nums <- unlist(lapply(DF2, is.numeric)) 
DFnum<- DF2[ , nums]


 cormat <- round(cor(DFnum),2)
#head(cormat)
melted_cormat <- melt(cormat)
#head(melted_cormat)

#ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  #geom_tile()+ggtitle("Pearson correlation coefficient matrix for all")
  
  
# Get upper triangle of the correlation matrix
  get_upper_tri <- function(cormat){
    cormat[lower.tri(cormat)]<- NA
    return(cormat)
  }


upper_tri <- get_upper_tri(cormat)
#upper_tri


# compare means of Department,Education,JobSatisfaction, RelationshipSatisfaction, WorkLifeBalance,  JobInvolvement, PerformanceRating and EnvironmentSatisfaction based on different JobRole
# TODO 1 compare means of JobInvolvement, PerformanceRating and EnvironmentSatisfaction based on different JobRole
par(las=2) 
par(mfrow=c(2,2))
  
library(ggplot2)

 #caculate the mean of DF2$Education for each category(DF2$JobRoleINT); aggregate by group and get mean
  EducationSTAT<-aggregate(DF2$Education, list(DF2$JobRole), mean)
summary(EducationSTAT)

#sort(EducationSTAT$x, decreasing = FALSE)

EducationSTATOrder<-EducationSTAT[order(EducationSTAT$x),c(1,2)]
EducationSTATOrder

names(EducationSTATOrder) <-c("JobRole", "Education")

ggplot(data=EducationSTATOrder, aes(x=reorder(JobRole,Education), y=Education, fill =JobRole)) +
geom_bar(stat="identity")+ coord_flip()+ geom_text(aes(label=Education), vjust=-0.5, size=3)+
  theme_minimal()+labs(title = "Fig - JobRole vs Education ",x="",y="")
  

  #caculate the mean of DF2$JobSatisfaction for each category(DF2$JobRoleINT); aggregate by group and get mean
JobSatisfactionSTAT<-aggregate(DF2$JobSatisfaction, list(DF2$JobRole), mean)
summary(JobSatisfactionSTAT)

#sort(JobSatisfactionSTAT$x, decreasing = FALSE)

JobSatisfactionSTATOrder<-JobSatisfactionSTAT[order(JobSatisfactionSTAT$x),c(1,2)]
JobSatisfactionSTATOrder

names(JobSatisfactionSTATOrder) <-c("JobRole", "JobSatisfaction")

ggplot(data=JobSatisfactionSTATOrder, aes(x=reorder(JobRole,JobSatisfaction), y=JobSatisfaction, fill =JobRole)) +
geom_bar(stat="identity")+ coord_flip()+ geom_text(aes(label=JobSatisfaction), vjust=-0.5, size=3)+
  theme_minimal()+labs(title = "Fig - JobRole vs JobSatisfaction ",x="",y="")


#caculate the mean of DF2$RelationshipSatisfaction for each category(DF2$JobRoleINT); aggregate by group and get mean
RelationshipSatisfactionSTAT<-aggregate(DF2$RelationshipSatisfaction, list(DF2$JobRole), mean)
summary(RelationshipSatisfactionSTAT)

#sort(RelationshipSatisfactionSTAT$x, decreasing = FALSE)

RelationshipSatisfactionSTATOrder<-RelationshipSatisfactionSTAT[order(RelationshipSatisfactionSTAT$x),c(1,2)]
RelationshipSatisfactionSTATOrder

names(RelationshipSatisfactionSTATOrder) <-c("JobRole", "RelationshipSatisfaction")

ggplot(data=RelationshipSatisfactionSTATOrder, aes(x=reorder(JobRole,RelationshipSatisfaction), y=RelationshipSatisfaction, fill =JobRole)) +
geom_bar(stat="identity")+ coord_flip()+ geom_text(aes(label=RelationshipSatisfaction), vjust=-0.5, size=3)+
  theme_minimal()+labs(title = "Fig - JobRole vs RelationshipSatisfaction ",x="",y="")

#caculate the mean of DF2$WorkLifeBalance for each category(DF2$JobRoleINT); aggregate by group and get mean
WorkLifeBalanceSTAT<-aggregate(DF2$WorkLifeBalance, list(DF2$JobRole), mean)
summary(WorkLifeBalanceSTAT)

#sort(WorkLifeBalanceSTAT$x, decreasing = FALSE)

WorkLifeBalanceSTATOrder<-WorkLifeBalanceSTAT[order(WorkLifeBalanceSTAT$x),c(1,2)]
WorkLifeBalanceSTATOrder

names(WorkLifeBalanceSTATOrder) <-c("JobRole", "WorkLifeBalance")

ggplot(data=WorkLifeBalanceSTATOrder, aes(x=reorder(JobRole,WorkLifeBalance), y=WorkLifeBalance, fill =JobRole)) +
geom_bar(stat="identity")+ coord_flip()+ geom_text(aes(label=WorkLifeBalance), vjust=-0.5, size=3)+
  theme_minimal()+labs(title = "Fig - JobRole vs WorkLifeBalance ",x="",y="")


#caculate the mean of DF2$JobInvolvement for each category(DF2$JobRoleINT); aggregate by group and get mean
JobInvolvementSTAT<-aggregate(DF2$JobInvolvement, list(DF2$JobRole), mean)
summary(JobInvolvementSTAT)

#sort(JobInvolvementSTAT$x, decreasing = FALSE)

JobInvolvementSTATOrder<-JobInvolvementSTAT[order(JobInvolvementSTAT$x),c(1,2)]
JobInvolvementSTATOrder

names(JobInvolvementSTATOrder) <-c("JobRole", "JobInvolvement")

ggplot(data=JobInvolvementSTATOrder, aes(x=reorder(JobRole,JobInvolvement), y=JobInvolvement, fill =JobRole)) +
geom_bar(stat="identity")+ coord_flip()+ geom_text(aes(label=JobInvolvement), vjust=-0.5, size=3)+
  theme_minimal()+labs(title = "Fig - JobRole vs JobInvolvement ",x="",y="")
 
#caculate the mean of DF2$PerformanceRating for each category(DF2$JobRoleINT); aggregate by group and get mean
PerformanceRatingSTAT<-aggregate(DF2$PerformanceRating, list(DF2$JobRole), mean)
summary(PerformanceRatingSTAT)

#sort(PerformanceRatingSTAT$x, decreasing = FALSE)

PerformanceRatingSTATOrder<-PerformanceRatingSTAT[order(PerformanceRatingSTAT$x),c(1,2)]
PerformanceRatingSTATOrder

names(PerformanceRatingSTATOrder) <-c("JobRole", "PerformanceRating")

ggplot(data=PerformanceRatingSTATOrder, aes(x=reorder(JobRole,PerformanceRating), y=PerformanceRating, fill =JobRole)) +
geom_bar(stat="identity")+ coord_flip()+ geom_text(aes(label=PerformanceRating), vjust=-0.5, size=3)+
  theme_minimal()+labs(title = "Fig - JobRole vs PerformanceRating ",x="",y="")

#caculate the mean of DF2$EnvironmentSatisfaction for each category(DF2$JobRoleINT); aggregate by group and get mean
EnvironmentSatisfactionSTAT<-aggregate(DF2$EnvironmentSatisfaction, list(DF2$JobRole), mean)
summary(EnvironmentSatisfactionSTAT)

#sort(EnvironmentSatisfactionSTAT$x, decreasing = FALSE)

EnvironmentSatisfactionSTATOrder<-EnvironmentSatisfactionSTAT[order(EnvironmentSatisfactionSTAT$x),c(1,2)]
EnvironmentSatisfactionSTATOrder

names(EnvironmentSatisfactionSTATOrder) <-c("JobRole", "EnvironmentSatisfaction")

ggplot(data=EnvironmentSatisfactionSTATOrder, aes(x=reorder(JobRole,EnvironmentSatisfaction), y=EnvironmentSatisfaction, fill =JobRole)) +
geom_bar(stat="identity")+ coord_flip()+ geom_text(aes(label=EnvironmentSatisfaction), vjust=-0.5, size=3)+
  theme_minimal()+labs(title = "Fig - JobRole vs EnvironmentSatisfaction ",x="",y="")
```

##Objective 3. Build a model to predict attrition.
####Four different models were built and compared, they are Naive Bayes, Random Forest, Custom logistic regression (built with the top 15 important features selected in previous steps) and XGBoost. XGBoost were perfromed in Python, so find the analysis in attached file gxboostCS2,ipynb.
###Fitting the Naive Bayes Model

```{r}
#Fitting the Naive Bayes model； Accuracy： 81.33%
dfTrain <-read.csv("C:/Users/N1110/Desktop/CaseStudy2-data.csv",header=TRUE)
dfVal <- read.csv("C:/Users/N1110/Desktop/CaseStudy2Validation.csv",header=TRUE)

#fits the model on only the training set
library(e1071)
Naive_Bayes_Model=naiveBayes( Attrition~., data=dfTrain)
#What does the model say? Print the model summary
#Naive_Bayes_Model

#uses that model to predict the labels of the test set
#Prediction on the dataset
dfPreds0=predict(Naive_Bayes_Model,dfVal)

#Confusion matrix to check accuracy
#ctable(NB_Predictions,dfVal0$Attrition)
library(caret)
confusionMatrix(table(dfPreds0,dfVal$Attrition))

cMatrix<-table(dfPreds0, dfVal$Attrition)

#par(mfrow=c(1 ,2))
plot(cMatrix, col="blue", ylab="Actual", xlab="Predicted",main="Confusion matrix of Naive Bayes model")


```

####Outputting the Naive Bayes Model

```{r}
#Outputting
dfPreds1=cbind(dfVal$ID, dfPreds0)
#make a dataframe with only the ids in one column and the labels in the second column
dfPreds=as.data.frame(dfPreds1)

names(dfPreds)<- c("ID", "Attrition")

#Outputting data-prints those ids and predictions to a csv file
write.csv(dfPreds,"C:/Users/N1110/Desktop/Case2PredictionsJiang.csv",row.names = FALSE)

# Our output file have 2, 1 instead of Yes and No. Is it ok?



```

####Fitting models Random Forest and Custom logistic regression (built with the top 15 important features selected in previous steps) 

```{r}
#Random Forest model，Accuracy： model1=85% 

dfTrain0 <-read.csv("C:/Users/N1110/Desktop/CaseStudy2-data.csv",header=TRUE)
dfVal0 <- read.csv("C:/Users/N1110/Desktop/CaseStudy2Validation.csv",header=TRUE)

library(randomForest)
# Create a Random Forest model with default parameters
model1 <- randomForest(Attrition~., data=dfTrain0, importance = TRUE)
model1


# Predicting on train set
predTrain <- predict(model1, dfVal0, type = "class")
# Checking classification accuracy
library(caret)
confusionMatrix(table(predTrain, dfVal0$Attrition))
cMatrix<-table(predTrain, dfVal0$Attrition)
plot(cMatrix,ylab="Actual", xlab="Predicted",main="Confusion matrix of Random Forest model")

# To check important variables
importance(model1)        
#varImpPlot(model1) 
varImpPlot(model1,type=1)      
 # Feature importance plot using Random Forest. type 1 the mean decrease in accuracy,type2 mean decrease in node impurity.
#we use type 1 here, The more the accuracy of the random forest decreases due to the exclusion (or permutation) of a single variable, 
#the more important that variable is deemed, and therefore variables with a large mean decrease in accuracy are more important for classification of the data.
#The mean decrease in Gini coefficient is a measure of how each variable contributes to the homogeneity of the nodes and leaves in the resulting random forest. ...
# Variables that result in nodes with higher purity have a higher decrease in Gini coefficient.
#RF is one of the predictive modeling algorithms perform feature importance and selection internally while constructing their mode
dfTrain <-read.csv("C:/Users/N1110/Desktop/CaseStudy2-data.csv",header=TRUE)
dfVal <- read.csv("C:/Users/N1110/Desktop/CaseStudy2Validation.csv",header=TRUE)

dfTrain.train <- dfTrain

dfTrain.test <- dfVal


#Custom Logistic Regression (built with the top 15 important features selected in previous steps)
#modelCUSTOM1 <- glm(Attrition~., data = dfTrain.train, family=binomial)

modelCUSTOM <- glm(Attrition~OverTime +MaritalStatus +JobInvolvement +  YearsInCurrentRole+  MonthlyIncome+StockOptionLevel+Age  +  EnvironmentSatisfaction  + WorkLifeBalance + JobSatisfaction  + Department  + DistanceFromHome  +  JobRole  + TrainingTimesLastYear +  NumCompaniesWorked, data = dfTrain.train, family=binomial)


predictfun <- function(fit, test)

{
p <- ifelse(predict(fit, test, type="response") > 0.5, "Yes", "No")
 p
}
#Tests the models


predCUSTOM <- as.vector(predictfun(modelCUSTOM, dfTrain.test))


#Creates the confusion matrix with all those stats
library(caret)
COSTUMcMatrix <- confusionMatrix(table(dfTrain.test$Attrition, predCUSTOM))
COSTUMcMatrix 

plot(table(dfTrain.test$Attrition, predCUSTOM),col="green", ylab="Actual", xlab="Predicted",main="Confusion matrix of Custom model")
```

####Compare the accuracy of the four prediction methods, including XGBOOST that performed in python.

```{r}
# Create the data for the chart
# Create the data for the chart
H <- c(0.8133,0.8567,0.8633,0.8933)
M <- c("Naive Bayes","Random Forest","Custom", "XGBoost")

# Fitting Labels
par(las=2) # make label text perpendicular to axis
op <- par(mar=c(11,4,4,2)) # the 10 allows the names.arg below the barplot

# Plot the bar chart  

xx <-barplot(H,names.arg=M,xlab="",ylab="Accuracy",ylim=c(0,1), col=c("green","skyblue", "blue","red"),main="Accuracy Comparison of Prediction Methods",border="white")


			  ## Add text at top of bars
text(x = xx, y = H, label = H, pos = 3, cex = 0.8, col = "black")
```

##Conclusion
###The following tasks were completed in this study:
###Identified the top factors that contribute to turnover as following:
##### OverTime 
##### StockOption
##### MonthlyIncome


###Identified interesting job role specific trends and observations from the analysis, summarized here, on average:
####Research Directors have the highest Education level; while the Sales Representatives have the lowest Education level.
####Research Scientists have the highest job satisfaction; while the Human Resources have the lowest job satisfaction.
####Managers have the highest relationship satisfaction; while the Sales Representatives have the lowest relationship satisfaction.
####Human Resources have the highest work life balance; while the Research Scientist have the lowest.
####Research Scientists have the highest job involvement; while the Manufacturing Director have the lowest.
####Manufacturing Director have the highest performance rating; while the Research Director have the lowest.
####Manufacturing Director also have the highest environment satisfaction; while the Research Director have the lowest.

#### An interesting finding is Research Scientist lowest work life balance, but highest job satisfaction. On the contrast, Human Resources have the highest work life balance but the lowest job satisfaction.
#### We also found that although Lab Technician roles account for almost 20% of all Job Roles, the attrition seems to be abnormally high. In fact, the highest in the company. Further exploratory analysis may be needed to understand why that may be.


###Built models to predict attrition, with high accuracies, summarized here:
####Naive Bayes 81.33% Accuracy
####Random Forest: 85.67% Accuracy
####Custom Logistic Regression: 86.33% Accuracy
####XGBoost: 89.33% Accuracy

##Outputting data:
####Case2PredictionsJiang.csv contained the prediction results.

##Link to Youtube Presentation:
#CHANGE